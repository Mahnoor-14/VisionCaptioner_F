# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_bMrXqm5CDmQRO0-4JgovEWKYY_UUKrf
"""

import torch
import torch.nn as nn
import torchvision.models as models
import torchvision.transforms as T
from transformers import AutoTokenizer
from PIL import Image
import gradio as gr
# -----------------------------
# Model Definitions
# -----------------------------
class EncoderCNN(nn.Module):
    def __init__(self, embed_size):
        super(EncoderCNN, self).__init__()
        resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)
        modules = list(resnet.children())[:-1]  # remove fc
        self.resnet = nn.Sequential(*modules)
        self.fc = nn.Linear(resnet.fc.in_features, embed_size)
        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)

    def forward(self, images):
        with torch.no_grad():
            features = self.resnet(images)
        features = features.view(features.size(0), -1)
        features = self.fc(features)
        features = self.bn(features)
        return features

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super(PositionalEncoding, self).__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() *
                             (-torch.log(torch.tensor(10000.0)) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)

    def forward(self, x):
        return x + self.pe[:, :x.size(1), :]

class DecoderTransformer(nn.Module):
    def __init__(self, embed_size, vocab_size, num_heads=8, num_layers=6, dropout=0.1):
        super(DecoderTransformer, self).__init__()
        self.embed = nn.Embedding(vocab_size, embed_size)
        self.pos_encoder = PositionalEncoding(embed_size)
        decoder_layer = nn.TransformerDecoderLayer(d_model=embed_size, nhead=num_heads, dropout=dropout)
        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)
        self.fc_out = nn.Linear(embed_size, vocab_size)

    def forward(self, features, captions):
        embeddings = self.embed(captions)
        embeddings = self.pos_encoder(embeddings)
        embeddings = embeddings.transpose(0, 1)
        features = features.unsqueeze(0)
        tgt_mask = nn.Transformer.generate_square_subsequent_mask(embeddings.size(0)).to(features.device)
        output = self.transformer_decoder(embeddings, features, tgt_mask=tgt_mask)
        output = self.fc_out(output)
        return output.transpose(0, 1)

# -----------------------------
# Load Checkpoint
# -----------------------------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

embed_size = 256
vocab_size = tokenizer.vocab_size
encoder = EncoderCNN(embed_size).to(device)
decoder = DecoderTransformer(embed_size, vocab_size).to(device)

CHECKPOINT_PATH = r"D:\Atom_Project\visioncaptioner\New folder\caption_model_epoch36.pth"

checkpoint = torch.load(CHECKPOINT_PATH, map_location=device)
encoder.load_state_dict(checkpoint["encoder_state_dict"])
decoder.load_state_dict(checkpoint["decoder_state_dict"])
encoder.eval()
decoder.eval()

# -----------------------------
# Image Transform
# -----------------------------
transform = T.Compose([
    T.Resize((224, 224)),
    T.ToTensor(),
])

# Caption Generator
# -----------------------------
def generate_caption(image, max_len=50):
    image = transform(image).unsqueeze(0).to(device)
    feature = encoder(image)
    caption = [tokenizer.cls_token_id]

    for _ in range(max_len):
        inputs = torch.tensor(caption).unsqueeze(0).to(device)
        outputs = decoder(feature, inputs)
        next_word = outputs.argmax(-1)[:, -1].item()
        caption.append(next_word)
        if next_word == tokenizer.sep_token_id:
            break

    return tokenizer.decode(caption, skip_special_tokens=True)
# 
# -----------------------------
# Gradio Frontend
# -----------------------------
demo = gr.Interface(
    fn=generate_caption,
    inputs=gr.Image(type="pil"),
    outputs="text",
    title="VisionCaptioner",
    description="Upload an image to generate an AI-based caption.",
    theme="glass"   # try "default", "glass", "monochrome", etc.
)


if __name__ == "__main__":
    demo.launch(share=True)